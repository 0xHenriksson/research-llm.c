Title: Byte Latent Transformer: Patches Scale Better Than Tokens

Abstract:
Abstract
We introduce the Byte Latent Transformer
                            (BLT), a new byte-level LLM
                            architecture that, for the first time, matches tokenization-based LLM performance at scale
                            with significant improvements in inference efficiency and robustness.
                            BLT encodes bytes into
                            dynamically sized patches, which serve as the primary units of computation.
                            Patches are segmented based on the entropy of the next byte, allocating more compute and
                            model capacity where increased data complexity demands it.
                            We present the first flop
                            controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our
                            results demonstrate the feasibility of scaling models trained on raw bytes without a fixed
                            vocabulary.
                            Both training and inference efficiency improve due to dynamically selecting long patches
                            when data is predictable, along with qualitative improvements on reasoning and long tail
                            generalization. Overall, for fixed inference costs, BLT shows significantly better
                            scaling than tokenization-based models, by simultaneously growing both patch and model
                            size.


1 Introduction

We introduce the Byte Latent Transformer (BLT), a tokenizer-free architecture
                            that learns from raw byte data and, for the first time, matches the performance of
                            tokenization-based models at scale, with significant improvements in efficiency and
                            robustness (¬ß6).
                            Existing large language models (llms) are trained almost entirely end-to-end, except for
                            tokenization‚Äîa heuristic pre-processing step that groups bytes into a static set of tokens.
                            Such tokens bias how a string is compressed, leading to shortcomings such as domain/modality
                            sensitivity,
                            sensitivity to input noise (¬ß6), a lack of orthographic knowledge, and
                            multilingual inequity.

Tokenization has previously been essential because directly
                            trainingllms on bytes is
                            prohibitively costly at scale due to long sequence lengths.
                            Prior works mitigate this by employing more efficient self-attentionor attention-free architectures(¬ß8). However, this primarily helps trainsmall models.
                            At scale, the computational cost of a Transformer is dominated by large feed-forward network
                            layers that run on every byte, not the cost of the attention mechanism.

To efficiently allocate compute, we propose a dynamic, learnable
                            method for grouping bytes intopatches(¬ß2) and a new model architecture that mixes
                            byte and patch information.
                            Unlike tokenization,BLT has
                            no fixed vocabulary for patches.
                            Arbitrary groups of bytes are mapped to latent patch representations via light-weight
                            learned encoder and decoder modules.
                            We show that this results inmoreefficient allocation of compute than tokenization-based
                            models.

Tokenization-basedllms allocate the same amount of compute to every token. This
                            trades efficiency for performance, since tokens are induced with compression heuristics that
                            are not always correlated with the complexity of predictions. Central to our architecture is
                            the idea that models should dynamically allocate compute where it is needed. For example, a
                            large transformer is not needed to predict the ending of most words, since these are
                            comparably easy, low-entropy decisions compared to choosing the first word of a new
                            sentence.
                            This is reflected inBLT‚Äôs
                            architecture (¬ß3) where there are three transformer blocks:
                            two small byte-levellocal
                                modelsand a large globallatent transformer(Figure 2).
                            To determine how to group bytes into patches and therefore how to dynamically allocate
                            compute,BLT
                            segments data based on the entropy of the next-byte prediction creating contextualized
                            groupings of bytes with relatively uniform information density.

We present the firstflop-controlled scaling study of byte-level models up to 8B
                            parameters and 4T training bytes, showing that we can train a model end-to-end at scale from
                            bytes without fixed-vocabulary tokenization.
                            Overall,BLT matches
                            trainingflop-controlled
                            performanceof Llama 3 while using up to 50% fewerflops at inference (¬ß5).
                            We also show that directly working with raw bytes provides significant improvements in
                            modeling the long-tail of the data.BLT models are more robust than tokenizer-based models to noisy
                            inputs and display enhanced character level understanding abilities demonstrated on
                            orthographic knowledge, phonology, and low-resource machine translation tasks (¬ß6).
                            Finally, withBLT models, we
                            can simultaneously increase model size and patch size while maintaining the same inferenceflopbudget. Longer patch
                            sizes, on average, save compute which can be reallocated to grow the size of the global
                            latent transformer, because it is run less often. We conduct inference-flopcontrolled scaling
                            experiments (Figure 1), and observe significantly better
                            scaling trends than with tokenization-based architectures.

In summary, this paper makes the following contributions:
                            1) We introduceBLT, a byte
                            latentllmarchitecture that
                            dynamically allocates compute to improveflopefficiency,
                            2) We show that we achieve trainingflop-controlled parity with Llama 3 up to 8B scale while having
                            the option to trade minor losses in evaluation metrics forflopefficiency gains of up to
                            50%, 3)BLT models unlock a
                            new dimension for scalingllms, where model size can now be scaled while maintaining a
                            fixed-inference budget, 4) We demonstrate the improved robustness ofBLT models to input noise and
                            their awareness of sub-word aspects of input data that token-basedllms miss.
                            We release the training and inference code forBLT athttps://github.com/facebookresearch/blt.


2 Patching: From Individual Bytes to Groups of
                        Bytes

Segmenting bytes intopatchesallowsBLT to dynamically allocate compute based on context.
                            Figure3shows several different methods for
                            segmenting bytes into patches.
                            Formally, a patching function f
p



                                                subscript
ùëì
ùëù


f_p
italic_f
                                        start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT segments a sequence of bytes ùíô
=

{

x
i

,
|
                                            
i
=
1
,
‚Ä¶
n
}



                                        \boldsymbol{x}=\{x_i,|i=1,\ldots n\}
bold_italic_x = {
                                        italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , | italic_i = 1 , ‚Ä¶
                                        italic_n } of length n
ùëõ

n
italic_n into a sequence of m
<
n



ùëö
ùëõ


m<n
italic_m <
                                        italic_n patches ùíë
=

{

p
                                                
j
                                                

|

j
                                                
=
                                                

1
,
‚Ä¶
,
m


}




ùíë

conditional-set

subscript

                                                        ùëù

                                                        ùëó





                                                        ùëó

1
‚Ä¶
ùëö






                                        \boldsymbol{p}=\{p_j|j=1,\ldots,m\}
bold_italic_p = {
                                        italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_j = 1 , ‚Ä¶ ,
                                        italic_m } by mapping each x
i



                                                subscript
ùë•
ùëñ


x_i
italic_x
                                        start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to the set {0,1} where 1 indicates the start of a new patch.
                            For both token-based and patch-based models, the computational cost of processing data is
                            primarily determined by the number of steps executed by the main Transformer. InBLT, this is the number of
                            patches needed to encode the data with a given patching function.
                            Consequently, the average size of a patch, or simplypatch size, is the main factor for determining the cost of
                            processing data during both training and inference with a given patching function (¬ß4.5).
                            Next, we introduce three patching functions: patching with a fixed number of bytes per patch
                            (¬ß2.1), whitespace patching (¬ß2.2), and dynamically patching with
                            entropies from a small bytelm(¬ß2.3). Finally, we discuss incremental
                            patching and how tokenization is different from patching (¬ß2.4).

Perhaps the most straightforward way to group bytes is
                                into patches of fixed size k
ùëò

k
italic_k as done in MegaByte.
                                The fixed stride is easy to implement for training and inference, provides a
                                straightforward mechanism for changing the average patch size, and therefore makes it
                                easy to control theflopcost.
                                However, this patching function comes with significant downsides.
                                First, compute is not dynamically allocated to where it is needed most: one could be
                                either wasting a transformer step j
ùëó

j
italic_j if only predicting whitespace in code, or not allocating sufficient compute for
                                bytes dense with information such as math.
                                Second, this leads to inconsistent and non-contextual patching of similar byte
                                sequences, such as the same word being split differently.

proposes a simple yet effective
                                improvement over strided patching that creates new patches after any space-like
                                byteswhich are natural boundaries for linguistic units in many languages.
                                In Space patching, a latent transformer step (i.e., moreflops) is allocated to
                                model every word.
                                This ensures words are patched in the same way across sequences and that flops are
                                allocated for hard predictions which often follow spaces. For example, predicting the
                                first byte of the answer to the question ‚ÄúWho composed the Magic Flute?‚Äù is much harder than
                                predicting the remaining bytes after ‚ÄúM‚Äù since the first character significantly reduces
                                the number of likely choices, making the completion ‚ÄúMozart‚Äù comparatively easy to
                                predict.
                                However, space patching cannot gracefully handle all languages and domains, and most
                                importantly cannot vary the patch size.
                                Next, we introduce a new patching method that uses the insight that the first bytes in
                                words are typically most difficult to predict, but that provides a natural mechanism for
                                controlling patch size.

Rather than relying on a rule-based heuristic such as
                                whitespace, we instead take a data-driven approach to identify high uncertainty
                                next-byte predictions.
                                We introduceentropy
                                    patching, which uses entropy estimates to derive patch boundaries.

We train a small byte-level auto-regressive language model
                                on the training data forBLT and compute next byte entropies under the LM
                                distribution p
e


subscript
ùëù
ùëí


p_e
                                        
italic_p
                                            start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT over the byte vocabulary ùí±
ùí±

V
                                        

                                            caligraphic_V :

We experiment with two methods to identify patch
                                boundaries given entropies H
‚Å¢

(

x
i

)





ùêª

subscript
ùë•
ùëñ



H(x_i)
                                        
italic_H (
                                            italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) . The first, finds points above a global entropy threshold, as illustrated inFigure 4. The second, identifies points
                                that are high relative to the previous entropy. The second approach can also be
                                interpreted as identifying points that break approximate monotonically decreasing
                                entropy withing the patch.

Patch boundaries are identified during a lightweight
                                preprocessing step executed during dataloading. This is different fromwhere classifier is trained to predict entropy-based patch boundaries.
                                In our experiments (¬ß4), we compare these two methods for
                                distinguishing between low and high entropy bytes.

Many modernllms, including our baseline Llama 3, use a subword
                                tokenizer likebpe.
                                We use ‚Äútokens‚Äù to refer to byte-groups drawn from afinitevocabulary
                                determined prior to training as opposed to ‚Äúpatches‚Äù which refer to dynamically grouped
                                sequences without a fixed vocabulary.
                                A critical difference between patches and tokens is that with tokens, the model has no
                                direct access to the underlying byte features.

A crucial improvement ofBLT over
                                tokenization-based models is that redefines the trade off between the vocabulary size
                                and compute. In standardllms, increasing the size of the vocabulary means larger
                                tokens on average and therefore fewer steps for the model but also larger output
                                dimension for the final projection layer of the model. This trade off effectively leaves
                                little room for tokenization based approaches to achieve significant variations in token
                                size and inference cost. For example, Llama 3 increases the average token size from 3.7
                                to 4.4 bytes at the cost of increasing the size of its embedding table 4x compared to
                                Llama 2.

When generating,BLT needs to decide whether the current step in the byte
                                sequence is at a patch boundary or not as this determines whether more compute is
                                invoked via the Latent Transformer. This decision needs to occur independently of the
                                rest of the sequence which has yet to be generated. Thus patching cannot assume access
                                to future bytes in order to choose how to segment the byte sequence. Formally, a
                                patching scheme f
p


subscript
ùëì
ùëù


f_p
                                        
italic_f
                                            start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT satisfies the property of incremental patching if it satisfies:

bpeis not an incremental patching scheme as the same
                                prefix can be tokenized differently depending on the continuation sequence, and
                                therefore does not satisfy the property above.


3 BLT Architecture

BLT is composed of a large global autoregressive language model
                            that operates on patch representations, along with two smaller local models that encode
                            sequences of bytes into patches and decode patch representations back into bytes (Figure 2).

The Latent Global Transformeris an autoregressive
                                transformer model ùí¢
ùí¢

G
                                        

                                            caligraphic_G with l
ùí¢


subscript
ùëô
ùí¢


l_G
                                        
italic_l
                                            start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT layers, which maps a sequence of latent input patch representations, p
j


subscript
ùëù
ùëó


p_j
                                        
italic_p
                                            start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT into a sequence of output patch representations, o
j


subscript
ùëú
ùëó


o_j
                                        
italic_o
                                            start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT . Throughout the paper, we use the subscript j
ùëó

j
italic_j to denote patches and i
ùëñ

i
italic_i to denote bytes. The global model uses a block-causal attention mask,
                                which restricts attention to be up to and including the current patch within the current
                                document. This model consumes the bulk of theflops during pre-training as well as inference, and thus,
                                choosing when to invoke it allows us to control and vary the amount of compute expended
                                for different portions of the input and output as a function of input/output complexity.

The Local Encoder Model, denoted by ‚Ñ∞
‚Ñ∞

E
                                        

                                            caligraphic_E , is a lightweight transformer-based model with l
                                                
‚Ñ∞

<<

l
                                                
ùí¢



much-less-than

subscript

                                                        ùëô
‚Ñ∞
                                                    


subscript

                                                        ùëô

                                                        ùí¢




                                            l_E<<l_G
italic_l
                                            start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT < < italic_l
                                            start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT layers, whose main role is to efficiently map a sequence of input bytes b
i


subscript
ùëè
ùëñ


b_i
                                        
italic_b
                                            start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , into expressive patch representations, p
j


subscript
ùëù
ùëó


p_j
                                        
italic_p
                                            start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT .
                                A primary departure from the transformer architecture is the addition of a
                                cross-attention layer after each transformer layer, whose function is to pool byte
                                representations into patch representations (Figure 5).
                                First, the input sequence of bytes, b
i


subscript
ùëè
ùëñ


b_i
                                        
italic_b
                                            start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , are embedded using a ‚Ñù

256
                                                
√ó

h
‚Ñ∞




superscript
‚Ñù


256

subscript
‚Ñé
‚Ñ∞





                                            R^256\times h_E

                                            blackboard_R start_POSTSUPERSCRIPT 256 √ó italic_h start_POSTSUBSCRIPT
                                            caligraphic_E end_POSTSUBSCRIPT end_POSTSUPERSCRIPT matrix, denoted as x
i


subscript
ùë•
ùëñ


x_i
                                        
italic_x
                                            start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .
                                These embeddings are then optionally augmented with additional information in the form
                                of hash-embeddings (¬ß3.2.1).
                                A series of alternating transformer and cross-attention layers (¬ß3.2.2) then transform these
                                representations into patch representations, p
i


subscript
ùëù
ùëñ


p_i
                                        
italic_p
                                            start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT that are processed by the global transformer, ùí¢
ùí¢

G
                                        

                                            caligraphic_G .
                                The transformer layers use alocal block causalattention mask; each byte attends to a
                                fixed window of w
‚Ñ∞


subscript
ùë§
                                                
‚Ñ∞
                                                



                                            w_E
italic_w
                                            start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT preceding bytes that in general can cross the dynamic patch boundaries but can
                                not cross document boundaries.
                                The following subsections describe details about the embeddings and the cross-attention
                                block.

A key component in creating robust, expressive
                                    representations at each step i

ùëñ
                                                

i
                                            

                                                italic_i is to incorporate information about the preceding bytes.
                                    InBLT, we
                                    achieve this by modeling both the byte b
i


subscript
ùëè
ùëñ


b_i
                                            

                                                italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT individuallyandas part of a byte n-gram.
                                    For each step i

ùëñ
                                                

i
                                            

                                                italic_i , we first construct byte-grams

for each byte position i

ùëñ
                                                

i
                                            

                                                italic_i and n

ùëõ
                                                

n
                                            

                                                italic_n from three to eight.

We then introduce hash n

ùëõ
                                                

n
                                            

                                                italic_n -gram embeddings, that map all byte n

ùëõ
                                                

n
                                            

                                                italic_n -grams via a hash function to an index in an embedding table E
n

h
‚Å¢
a
‚Å¢
s
‚Å¢
h



superscript

subscript
ùê∏
ùëõ



‚Ñé
ùëé
ùë†
‚Ñé




                                                E_n^hash

                                                italic_E start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT
                                                start_POSTSUPERSCRIPT italic_h italic_a italic_s italic_h
                                                end_POSTSUPERSCRIPT with a fixed size, for each size n
‚àà

{
3
,
4
,
5
,
6
,
7
,
8
}




ùëõ

3
4
5
6
7
8




                                                n\in\{3,4,5,6,7,8\}

                                                italic_n ‚àà { 3 , 4 , 5 , 6 , 7 , 8 } .
                                    The resulting embedding is then added to the embedding of the byte before being
                                    normalized and passed as input to the local encoder model.
                                    We calculate the augmented embedding

We normalize e
i


subscript
ùëí
ùëñ


e_i
                                            

                                                italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT by the number of n

ùëõ
                                                

n
                                            

                                                italic_n -grams sizes plus one and use RollPolyHash as defined in Appendix13.
                                    In Section7, we ablate the effects of n

ùëõ
                                                

n
                                            

                                                italic_n -gram hash embeddings with different values for n

ùëõ
                                                

n
                                            

                                                italic_n and embedding table size onflop-controlled scaling law trends.
                                    In addition to hash n

ùëõ
                                                

n
                                            

                                                italic_n -gram embeddings, we also experimented with frequency based n
                                            

                                                    ùëõ

n
                                            

                                                italic_n -gram embeddings, and we provide details of this exploration in Appendix14.

We closely follow the input cross-attention
                                    module of the Perceiver architecture, with the main difference being that latent
                                    representations correspond to variable patch representations as opposed to a fixed
                                    set of latent representations (Figure 5), and only attend to the
                                    bytes that make up the respective patch.
                                    The module comprises a query vector, corresponding to each patch p
j


subscript
ùëù
ùëó


p_j
                                            

                                                italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , which is initialized by pooling the byte representations corresponding to
                                    patch p
j


subscript
ùëù
ùëó


p_j
                                            

                                                italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , followed by a linear projection, ‚Ñ∞
C

‚àà

‚Ñù


h
‚Ñ∞

√ó
                                                        

(



                                                                        h

                                                                        ‚Ñ∞

√ó


                                                                        U

                                                                        ‚Ñ∞


)







subscript
‚Ñ∞
ùê∂


superscript
‚Ñù



subscript
                                                                
‚Ñé
‚Ñ∞





                                                                        subscript
‚Ñé
                                                                    
‚Ñ∞
                                                                    



                                                                        subscript
ùëà
                                                                    
‚Ñ∞
                                                                    







                                                E_C\inR^h_E\times(h_E\times
                                                U_%
                                                E)

                                                caligraphic_E start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ‚àà
                                                blackboard_R start_POSTSUPERSCRIPT italic_h start_POSTSUBSCRIPT
                                                caligraphic_E end_POSTSUBSCRIPT √ó ( italic_h start_POSTSUBSCRIPT
                                                caligraphic_E end_POSTSUBSCRIPT √ó italic_U start_POSTSUBSCRIPT
                                                caligraphic_E end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT , where U
‚Ñ∞


subscript
ùëà
‚Ñ∞



                                                U_E

                                                italic_U start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT is the number of encoder cross-attention heads.
                                    Formally, if we let f
bytes

‚Å¢

(

p
j

)





subscript
ùëì

bytes



subscript
ùëù
ùëó




                                                f_bytes(p_j)

                                                italic_f start_POSTSUBSCRIPT bytes end_POSTSUBSCRIPT ( italic_p
                                                start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) denote the sequence of bytes corresponding to patch, p
j


subscript
ùëù
ùëó


p_j
                                            

                                                italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , then we calculate

where P
‚àà

‚Ñù


n
p

√ó
                                                        

h
ùí¢






ùëÉ

superscript
‚Ñù



subscript
                                                                
ùëõ
ùëù


subscript
                                                                
‚Ñé
ùí¢






                                                P\inR^n_p\times h_G

                                                italic_P ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n
                                                start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT √ó italic_h
                                                start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT end_POSTSUPERSCRIPT represents n
p


subscript
ùëõ
ùëù


n_p
                                            

                                                italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT patch representations to be processed by the global model, which is
                                    initialized by pooling together the byte embeddings e
i


subscript
ùëí
ùëñ


e_i
                                            

                                                italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT corresponding to each patch p
j


subscript
ùëù
ùëó


p_j
                                            

                                                italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT . W
q


subscript
ùëä
ùëû


W_q
                                            

                                                italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , W
k


subscript
ùëä
ùëò


W_k
                                            

                                                italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , W
v


subscript
ùëä
ùë£


W_v
                                            

                                                italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and W
o


subscript
ùëä
ùëú


W_o
                                            

                                                italic_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT are the projections corresponding to the queries, keys, values, and output
                                    where the keys and values are projections of byte representations h
i


subscript
‚Ñé
ùëñ


h_i
                                            

                                                italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT from the previous layer ( e
i


subscript
ùëí
ùëñ


e_i
                                            

                                                italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for the first layer).
                                    We use a masking strategy specific to patching where each query Q
j


subscript
ùëÑ
ùëó


Q_j
                                            

                                                italic_Q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT only attends to the keys and values that correspond to the bytes in patch j
                                            
ùëó

j
                                            

                                                italic_j . Because we use multi-headed attention over Q
,
K


ùëÑ
ùêæ


Q,K
                                            

                                                italic_Q , italic_K and V
                                            
ùëâ

V
                                            

                                                italic_V and patch representations are typically of larger dimension ( h
ùí¢


subscript
‚Ñé
ùí¢



                                                h_G

                                                italic_h start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT ) than h
‚Ñ∞


subscript
‚Ñé
‚Ñ∞



                                                h_E

                                                italic_h start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT , we maintain P
l


subscript
ùëÉ
ùëô


P_l
                                            

                                                italic_P start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT as multiple heads of dimension h
‚Ñ∞


subscript
‚Ñé
‚Ñ∞



                                                h_E

                                                italic_h start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT when doing cross-attention, and later, concat these representations into h
ùí¢


subscript
‚Ñé
ùí¢



                                                h_G

                                                italic_h start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT dimensions.
                                    Additionally, we use a pre-LayerNorm on the queries, keys and values and no
                                    positional embeddings are used in this cross-attention module.
                                    Finally, we use a residual connection around the cross-attention block.

Similar to the local encoder, the local decoder ùíü
ùíü

D
                                        

                                            caligraphic_D is a lightweight transformer-based model with l
                                                
ùíü

<<

l
                                                
ùí¢



much-less-than

subscript

                                                        ùëô

                                                        ùíü


subscript

                                                        ùëô

                                                        ùí¢




                                            l_D<<l_G
italic_l
                                            start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT < < italic_l
                                            start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT layers, that decodes a sequence of global patch representations o
j


subscript
ùëú
ùëó


o_j
                                        
italic_o
                                            start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , into raw bytes, y
i


subscript
ùë¶
ùëñ


y_i
                                        
italic_y
                                            start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . The local decoder predicts a sequence of raw bytes, as a function of previously
                                decoded bytes, and thus, takes as input the hidden representations produced by the local
                                encoder for the byte-sequence.
                                It applies a series of l
ùíü


subscript
ùëô
ùíü


l_D
                                        
italic_l
                                            start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT alternating layers of cross attention and transformer layers. The
                                cross-attention layer in the decoder is applied before the transformer layer to first
                                create byte representations from the patch representations, and the local decoder
                                transformer layer operates on the resulting byte sequence.

In the decoder cross-attention, the roles of the
                                    queries and key/values are interchanged i.e. the byte-representations are now the
                                    queries, and the patch representations are now the key/values. The initial
                                    byte-representations for the cross-attention are initialized as the byte embeddings
                                    from the last encoder layer i.e. h

l
‚Ñ∞



subscript
‚Ñé

subscript
ùëô
‚Ñ∞




                                                h_l_{E}

                                                italic_h start_POSTSUBSCRIPT italic_l start_POSTSUBSCRIPT caligraphic_E
                                                end_POSTSUBSCRIPT end_POSTSUBSCRIPT . The subsequent byte-representations for layer l

ùëô
                                                

l
                                            

                                                italic_l , d

l
,
i



subscript
ùëë

ùëô
ùëñ



d_l,i
                                            

                                                italic_d start_POSTSUBSCRIPT italic_l , italic_i end_POSTSUBSCRIPT are computed as:

where once again, W
k

,

W
v




subscript
ùëä
ùëò


subscript
ùëä
ùë£




                                                W_k,W_v

                                                italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_W
                                                start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT are key/value projection matrices that operate on a linear transformation
                                    and split operation ùíü
C


subscript
ùíü
ùê∂



                                                D_C

                                                caligraphic_D start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT , applied to the final patch representations o
j


subscript
ùëú
ùëó


o_j
                                            

                                                italic_o start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT from the global model, W
q


subscript
ùëä
ùëû


W_q
                                            

                                                italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT is a query projection matrices operating on byte representations d

l
‚àí
1



subscript
ùëë


ùëô
1



d_l-1
                                            

                                                italic_d start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT from the previous decoder transformer layer (or h

l
‚Ñ∞



subscript
‚Ñé

subscript
ùëô
‚Ñ∞




                                                h_l_{E}

                                                italic_h start_POSTSUBSCRIPT italic_l start_POSTSUBSCRIPT caligraphic_E
                                                end_POSTSUBSCRIPT end_POSTSUBSCRIPT for the first layer), and W
o


subscript
ùëä
ùëú


W_o
                                            

                                                italic_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT is the output projection matrix, thus making B
‚àà

‚Ñù


h
ùíü

√ó
                                                        

n
b






ùêµ

superscript
‚Ñù



subscript
                                                                
‚Ñé
ùíü


subscript
                                                                
ùëõ
ùëè






                                                B\inR^h_D\times n_b

                                                italic_B ‚àà blackboard_R start_POSTSUPERSCRIPT italic_h
                                                start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT √ó italic_n
                                                start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , where n
b


subscript
ùëõ
ùëè


n_b
                                            

                                                italic_n start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT is the number of output bytes. The next decoder representations D
l


subscript
ùê∑
ùëô


D_l
                                            

                                                italic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT are computed using a decoder transformer layer on the output of the
                                    cross-attention block, B
                                            
ùêµ

B
                                            

                                                italic_B . As in the local encoder cross-attention, we use multiple heads in the
                                    attention, use pre LayerNorms, no positional embeddings, and a residual connection
                                    around the cross-attention module.


4 Experimental Setup

We carefully design controlled experiments to compareBLT with tokenization based
                            models with particular attention to not giveBLT any advantages from possibly using longer sequence contexts.

All model scales that we experiment in this paper are
                                pre-trained on two datasets: 1) The Llama 2 dataset,
                                which comprises 2 trillion tokens collected from a variety of publicly available
                                sources, which are subsequently cleaned and filtered to improve quality; and 2)BLT-1T: A new dataset
                                with 1 trillion tokens gathered from various public sources, and also including a subset
                                of the pre-training data released by Datacomp-LM.
                                The former is used for scaling law experiments on optimal number of tokens as determined
                                byto determine the best architectural choices forBLT, while the latter
                                is used for a complete pre-training run to compare with Llama 3 on downstream tasks.
                                Neither of these datasets include any data gathered from Meta products or services.
                                Furthermore, for baseline experiments for tokenizer-based models, we use the Llama 3
                                tokenizer with a vocabulary size of 128K tokens, which produced stronger baseline
                                performance that the Llama 2 tokenizer in our experiments.

The entropy model in our experiments is a byte level
                                language model trained on the same training distribution as the fullBLT model. Unless
                                otherwise mentioned, we use a transformer with 100M parameters, 14 layers, and a hidden
                                dimensionality of 512, and sliding window attention of 512 bytes. The remaining
                                hyperparameters are the same as in our local and global transformers. We experimented
                                with different model sizes, receptive fields, and architectures as discussed insection 7. In particular, when the
                                receptive field of the model is small enough, the trained entropy model can be encoded
                                in an efficient lookup table.

For models using entropy-based patching, we estimate a
                                patching threshold that achieves a desired averagepatch sizeon the
                                pretraining data mix. InBLT, unlike with tokenization, thepatch sizecan be
                                arbitrarily chosen having significant implications on the context size used by the
                                model.
                                To maintain the same average context length and avoid giving larger patch sizes unfair
                                advantage, we ensure that the number of bytes in each batch remains constant in
                                expectation. This means that we reduce the sequence length of models with larger patch
                                sizes. On Llama 2 data, we use a 8k byte context while on theBLT-1T dataset we
                                increase the context to 16k bytes on average while maintaining the same batch size of
                                16M bytes on average.

While the average batch size is constant, when loading
                                batches of data, dynamic patching methods yield different ratios of bytes to patches.
                                For efficiency reasons, our implementation ofBLT training packs batches of patches to avoid padding
                                steps in the more expensive latent transformer. This ensures that every batch has the
                                same number of patches. During training we pad and possibly truncate byte sequences to
                                12k and 24k bytes respectively for Llama 2 andBLT-1T datasets, to avoid memory spikes from sequences
                                with unusually large patches.

Empirically, we find that using entropy patching yields
                                progressively larger patches in structured content like multiple choice tasks (see
                                patching on an MMLU example inFigure 9) which are often very
                                repetitive. These variations are caused by lower entropy on the repeated content found
                                in the entropy model context. So for the large scale run ofBLT-Entropy with patch
                                size 4.5, we reset the entropy context with new lines and use approximate monontonicity
                                constraint as it suffers less from "entropy drift" from changes in context length. This
                                change only affects how we compute entropies, but we still follow the same procedure to
                                identify the value of the entropy threshold.

We largely follow the equations for computation of
                                transformerflops
                                from Chinchillacomprisingflops for the feed-forward layers,qkvoprojections in
                                the self-attention layer, and computation of attention and output projection.
                                A notable difference is that we assume the input embedding layer is implemented as an
                                efficient lookup instead of a dense matrix multiplication, therefore becoming a 0-flopoperation.
                                Following previous work, we estimate that the backwards pass has twice the number offlops as the forward
                                pass.

To computeflopsper byteforBLT models, we add up theflops for the local
                                encoder transformer, the global latent transformer, and the local decoder transformer,
                                together with the cross attention blocks in the encoder and the decoder:

where n

c
                                                
‚Å¢
                                                
t
                                                
‚Å¢
                                                
x
                                                



subscript
ùëõ



                                                        ùëê

                                                        ùë°

                                                        ùë•



n_ctx
                                        
italic_n
                                            start_POSTSUBSCRIPT italic_c italic_t italic_x end_POSTSUBSCRIPT is the sequence length in bytes, n
p


subscript
ùëõ
ùëù


n_p
                                        
italic_n
                                            start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT is the patch size, r
ùëü

r
italic_r is the ratio of queries to key/values, k
ùëò

k
italic_k is the ratio of patch-dimension to byte-dimension i.e. the number of local model
                                splits that concatenate to form a global model representation ( k
=
2



ùëò
2


k=2
italic_k =
                                            2 inFigure 5). V
ùëâ

V
italic_V corresponds to the vocabulary size for the output projection, which is only used
                                in the local decoder.
                                Depending on whether a module is applied on the byte or patch sequence, the attention
                                uses a different context length, m
ùëö

m
italic_m . We modify the attentionflops accordingly for each component.
                                The exact equations forflops computation for Transformer-FLOPs and
                                Cross-Attention FLOPs are provided in Appendix12.

Perplexity only makes sense in the context of a fixed
                                tokenizer as it is a measure of the uncertainty for each token. When comparing byte and
                                token-level models, following previous work,
                                we instead report Bits-Per-Byte (BPB), a tokenizer independent version of perplexity.
                                Specifically:

where the uncertainty over the data ùíô

ùíô

\boldsymbol{x}
                                        

                                            bold_italic_x as measured by the sum of the cross-entropy loss is normalized by the total
                                number of bytes in ùíô

ùíô

\boldsymbol{x}
                                        

                                            bold_italic_x and a constant.

For all the transformer blocks inBLT, i.e. both local
                                and global models, we largely follow the architecture of Llama 3;
                                we use the SwiGLU activation functionin the feed-forward layers, rotary positional embeddings (RoPE)with Œ∏
=
500000



ùúÉ
500000


theta=500000
                                        
italic_Œ∏ =
                                            500000 only in self-attention layers, and RMSNormfor layer normalization. We use Flash attentionfor all self-attention layers that use fixed-standard attention masks such asblock causalorfixed-window block
                                    causal, and a window size of 512 for fixed-width attention masks. Since our
                                cross-attention layers involve dynamic patch-dependent masks, we use Flex Attentionto produce fused implementations and significantly speed up training.

To study the effectiveness ofBLT models, we conduct
                                experiments along two directions, scaling trends, and downstream task evaluations, and
                                we consider models at different scales: 400M, 1B, 2B, 4B and 8B for these experiments.
                                The architecture hyperparameters for these models are presented in Appendix Table10.
                                We use max-pooling to initialize the queries for the first cross-attention layer in the
                                local encoder. We use 500
,
000


500
000


500,000
                                        
500 , 000 hashes with a single hash function, with n-gram sizes ranging from 3 to 8, for
                                allBLT models. We
                                use a learning rate of 4
                                                
‚Å¢
                                                
e
                                                

‚àí
4






4

                                                        ùëí

4


4e-4
                                        
4 italic_e
                                            - 4 for all models. The choice of matching learning rate between token andBLT models follows a
                                hyperparameter search between 1
                                                
‚Å¢
                                                
e
                                                

‚àí
3






1

                                                        ùëí

3


1e-3
                                        
1 italic_e
                                            - 3 and 1
                                                
‚Å¢
                                                
e
                                                

‚àí
4






1

                                                        ùëí

4


1e-4
                                        
1 italic_e
                                            - 4 at 400M and 1B model scales showing the same learning rate is optimal.
                                For scaling trends on Llama-2 data, we use training batch-sizes as recommended byor its equivalent in bytes. For optimization, we use the AdamW optimizerwith Œ≤
1


subscript
ùõΩ
1


beta_1
                                        
italic_Œ≤
                                            start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT set to 0.9 and Œ≤
2


subscript
ùõΩ
2


beta_2
                                        
italic_Œ≤
                                            start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT to 0.95, with an œµ
=

10
                                                


                                                        ‚àí
8





italic-œµ
                                                

superscript
10


8




\epsilon=10^-8
                                        
italic_œµ =
                                            10 start_POSTSUPERSCRIPT - 8 end_POSTSUPERSCRIPT . We use a linear warm-up of 2000 steps with an cosine decay schedule of the
                                learning rate to 0, we apply a weight decay of 0.1, and global gradient clipping at a
                                threshold of 1.0.


5 Scaling Trends

We present a holistic picture of the scaling trends of byte-level
                            models that can inform further scaling ofBLT models.
                            Our scaling study aims to address the limitations of previous research on byte-level models
                            in the following ways: (a) We compare trends for the compute-optimal training regime, (b) We
                            train matching 8B models on non-trivial amounts of training data (up to 1T tokens/4T bytes)
                            and evaluate on downstream tasks, and (c) We measure scaling trends in inference-cost
                            controlled settings.
                            In a later section, we will investigate specific advantages from modeling byte-sequences.

Using the Llama 2 dataset, we train variouscompute-optimalbpeandBLT models across four
                                different sizes, ranging from 1B to 8B parameters. We then plot the trainingflops against language
                                modeling performance on a representative subset of the training data mixture.
                                Thebpemodels are
                                trained using the optimal ratio of model parameters to training data, as determined by
                                Llama 3.
                                Thiscompute-optimalsetup is theoretically designed to achieve the best performance on the training dataset
                                within a given training budget, providing a robust baseline for our model.
                                For eachbpemodel,
                                we also train a correspondingBLT model on the same data, using a Latent Transformer
                                that matches the size and architecture of the correspondingbpeTransformer.

As illustrated inFigure 6(right),BLT models either
                                match or outperform theirbpecounterparts and this trend holds as we scale model
                                size andflops. To
                                the best of our knowledge,BLT is the first byte-level Transformer architecture to
                                achieve matching scaling trends with BPE-based models at compute optimal regimes. This
                                therefore validates our assumption that the optimal ratio of parameters to training
                                compute forbpealso
                                applies toBLT, or
                                at least it is not too far off.

Both architectural improvements and dynamic patching are
                                crucial to matchbpescaling trends. InFigure 6(left), we compare
                                space-patching-based models against Llama 3. We approximate SpaceByteusingBLT
                                space-patching without n-gram embeddings and cross-attention.
                                Although SpaceByte improves over Megabyte, it remains far from Llama 3. InFigure 6(right), we illustrate the
                                improvements from both architectural changes and dynamic patching.BLT models perform on
                                par with state-of-the-art tokenizer-based models such as Llama 3, at scale.

We also observe the effects of the choice of tokenizer on
                                performance for tokenizer-based models, i.e., models trained with the Llama-3 tokenizer
                                outperform those trained using the Llama-2 tokenizer on the same training data.

Finally, ourBLT architecture trends between Llama 2 and 3 when using
                                significantly larger patch sizes. Thebpetokenizers of Llama 2 and 3 have an average token size
                                of 3.7 and 4.4 bytes. In contrast,BLT can achieve similar scaling trends with an average
                                patch size of 6 and even 8 bytes. Inferenceflopare inversely proportional to the average patch size,
                                so using a patch size of 8 bytes would lead to nearly 50% inferenceflopsavings.
                                Models with larger patch sizes also seem to perform better as we scale model and data
                                size.BLT with patch
                                size of 8 starts at a significantly worse point compared tobpeLlama 2 at 1B but
                                ends up better thanbpeat 7B scale. This suggests that such patch sizes might
                                perform better at even larger scales and possibly that even larger ones could be
                                feasible as model size and training compute grow.

To assess scaling properties further, we train an 8BBLT model beyond the
                                compute optimal ratio on theBLT-1T dataset, a larger higher-quality dataset, and
                                measure performance on a suite of standard classification and generation benchmarks. For
                                evaluation, we select the following common sense reasoning, world knowledge, and code
                                generation tasks:

include ARC-Easy (0-shot), Arc-Challenge (0-shot), HellaSwag (0-shot), PIQA (0-shot), and MMLU (5-shot). We employ a prompt-scoring method, calculating
                                    the likelihood over choice characters, and report the average accuracy.

We report pass@1 scores on MBPP (3-shot)and HumanEval (0-shot), to evaluate the ability of LLMs to generate
                                    Python code.

InTable 1, we compare three models
                                    trained on theBLT-1T dataset: abpeLlama
                                    3 tokenizer-based model,and two variants of theBLT
                                    model. One employing a space-patching scheme (BLT-Space) and another utilizing an
                                    entropy-based patching scheme (BLT-Entropy). with approx. monotonicity
                                    constraint and reset the context of the entropy model with new lines (as discussed
                                    insubsection 4.4). All three models
                                    are trained with an equivalentflopbudget. However, withBLT-Entropy we additionally make an inference
                                    time adjustment of the entropy threshold from 0.6 to 0.1 which we find to improve
                                    task performance at the cost of more inference steps.

TheBLT-Entropy model outperforms the Llama 3
                                    model on 4 out of 7 tasks while being trained on the same number of bytes. This
                                    improvement is like due to a combination of (1) a better use of training compute via
                                    dynamic patching, and (2) the direct modeling of byte-level information as opposed
                                    to tokens.

On the other hand,BLT-Space
                                    underperforms the Llama 3 tokenizer on all but one task, but it achieves a
                                    significant reduction in inferenceflops with its larger average patch size of 6
                                    bytes.
                                    In comparison, thebpeand entropy-patching based models have
                                    roughly equivalent average patch size of approximately 4.5 bytes on the training
                                    data mix. With the same training budget, the larger patch size model covers 30% more
                                    data than the other two models which might pushBLT
                                    further away from the compute-optimal point.

WithBLT models, we can simultaneously increase model size and
                                patch size while maintaining the same training and inferenceflopbudget and
                                keeping the amount of training data constant. Arbitrarily increasing the patch size is a
                                unique feature of patch-based models which break free of the efficiency tradeoffs of
                                fixed-vocabulary token-based models, as discussed in Section2.4. Longer patch sizes save compute,
                                which can be reallocated to grow the size of the global latent transformer, because it
                                is run less often.

We conduct a fixed inference scaling study to test the
                                hypothesis that larger models taking fewer steps on larger patches might perform better
                                than smaller models taking more steps. Starting from model sizes of 400m and 3.6B
                                parameters with the Llama 2 tokenizer, we findflopequivalent models with the Llama 3 tokenizer andBLT-Entropy models
                                with average patch sizes of 6 and 8 bytes on the training datamix (seeTable 2for model details). For patch
                                size 8 models, we use 3 encoder layers instead of 1. We train each model for various
                                trainingflopbudgets.

Figure 1shows thatBLT models achieve
                                better scaling trends than tokenization-based architectures for both inferenceflopclasses. In both
                                cases, BPE models perform better with small training budgets and are quickly surpassed
                                byBLT, not far
                                beyond the compute-optimal regime. In practice, it can be preferable to spend more
                                during the one-time pretraining to achieve a better performing model with a fixed
                                inference budget. A perfect example of this is the class of 8B models, like Llama 3.1,
                                which has been trained on two orders of magnitude more data than what is compute-optimal
                                for that model size.

The crossover point whereBLT improves over
                                token-based models has shifted slightly closer to the compute-optimal point when moving
                                to the largerflopclass models (from 3x down to 2.5x the compute optimal budget). Similarly, the larger
                                patch size 8 model has steeper scaling trend in the largerflopclass overtaking
                                the other models sooner.
                                As discussed in Section5.1, larger patch sizes appear to
                                perform closer to BPE models at larger model scales. We attribute this, in part, to the
                                decreasing share of totalflops used by the byte-level Encoder and Decoder modules
                                which seem to scale slower than the Latent Transformer. When growing total parameters
                                20x from 400M to 8B, we only roughly doubleBLT‚Äôs local model parameters. This is important as larger
                                patch sizes only affectflops from the patch Latent Transformer and not the
                                byte-level modules. In fact, that is why theBLT-Entropy ps=8 went from 1.6x to 1.7x of the Llama 2
                                model size when moving to the larger model scale.

In summary, our patch-length scaling study demonstrates
                                that theBLT
                                patch-based architecture can achieve better scaling trends by simultaneously increasing
                                both patch and model size. Such trends seem to persist and even improve at larger model
                                scales.


6 Byte Modeling Improves Robustness

We also measure the robustness ofBLT compared to token-based
                            models that lack direct byte-level information, and present an approach to byte-ify
                            pretrained token-based models.

A very early motivation for training byte-level models was
                                to take advantage of their robustness to byte level noise in the input, and also to
                                exploit their awareness of the constituents of tokens, which current tokenizer-based
                                models struggle with. To measure these phenomena, we perform additional evaluations on
                                benchmarks that evaluate both robustness to input noise as well as awareness of
                                characters, both English and multi-lingual, including digits and phonemes. We present
                                these results in Table3.

We create noised versions of the benchmark
                                    classification tasks described in Section5.2, to compare the robustness of
                                    tokenizer-based models with that ofBLT.
                                    We employ five distinct character-level noising strategies to introduce variations
                                    in the text: (a)AntSpeak: This strategy converts the entire
                                    text into uppercase, space-separated characters. (b)Drop:
                                    Randomly removes 10% of the characters from the text. (c)RandomCase:
                                    Converts 50% of the characters to uppercase and 50% to lowercase randomly throughout
                                    the text. (d)Repeat: Repeats 20% of the characters up to a
                                    maximum of four times. (e)UpperCase: Transforms all characters in the
                                    text to uppercase.
                                    During evaluation, we apply each noising strategy to either the prompt, completion,
                                    or both as separate tasks and report the average scores. In Table3we report results on noised
                                    HellaSwagand find thatBLT
                                    indeed outperforms tokenizer-based models across the board in terms of robustness,
                                    with an average advantage of 8 points over the model trained on the same data, and
                                    even improves over the Llama 3.1 model trained on a much larger dataset.

We assessBLT‚Äôs
                                    capability to map a sequence of graphemes (characters representing a word) into a
                                    transcription of that word‚Äôs pronunciation (phonemes). In Table3, we present the results of the G2P
                                    task in a 5-shot setting using Phonology Benchand find thatBLT
                                    outperforms the baseline Llama 3 1T tokenizer-based model on this task.

To assess character-level understanding, we
                                    evaluateBLT on the CUTE benchmark, which comprises several tasks that are broadly
                                    classified into three categories: understanding composition, understanding
                                    orthographic similarity, and ability to manipulate sequences. This benchmark poses a
                                    significant challenge for most tokenizer-based models, as they appear to possess
                                    knowledge of their tokens‚Äô spellings but struggle to effectively utilize this
                                    information to manipulate text. Table3shows thatBLT-Entropy outperforms both BPE Llama 3
                                    models by more than 25 points on this benchmark. In particular, our model
                                    demonstrates exceptional proficiency in character manipulation tasks achieving 99.9%
                                    on both spelling tasks. Such large improvements despiteBLT
                                    having been trained on 16x less data than Llama 3.1 indicates that character level
                                    information is hard to learn for BPE models.
                                    Figure7illustrates a few such scenarios
                                    where Llama 3 tokenizer model struggles but ourBLT model
                                    performs well.
                                    Word deletion and insertion are the only two tasks where BPE performs better. Such
                                    word manipulation might not be straightforward for a byte-level model but the gap is
                                    not too wide and building from characters to words could be easier than the other
                                    way around. We use the same evaluation setup in all tasks and the original prompts
                                    from Huggingface. BPE models might benefit from additional prompt engineering.

We evaluateBLT on
                                    translating into and out of six popular language families and twenty one lower
                                    resource languages with various scripts from the FLORES-101 benchmarkand report SentencePiece BLEU in Table4.
                                    Our results demonstrate thatBLT outperforms a model trained with the
                                    Llama 3 tokenizer, achieving a 2-point overall advantage in translating into English
                                    and a 0.5-point advantage in translating from English. In popular language pairs,BLT
                                    performs comparably to or slightly better than Llama 3. However,BLT
                                    outperforms Llama 3 on numerous language pairs within lower-resource language
                                    families, underscoring the effectiveness of byte modeling for generalizing to
                                    long-tail byte sequences.

We explore a workflow whereBLT models can
                                leverage existing pre-trained tokenizer-based models for better and faster training
                                convergence, acheived by initializing the global transformer parameters ofBLT with those of a
                                pre-trained Llama 3.1 model. Subsequently, we update the weights of the global
                                transformer using one-tenth the learning rate employed for the local encoder and local
                                decoder model, for Llama 3 optimal number of steps, and present a comparison with a
                                baselineBLT in
                                Table5. It is evident thatBLT from Llama 3.1
                                significantly outperforms both the Llama 3 andBLT baselines, which were trained with the same number offlops. Moreover,
                                when compared to ourBLT-Entropy model (as presented in Table1), which was trained on a significantly
                                larger dataset (1T tokens),BLT from Llama 3.1 still achieves superior performance on
                                MMLU task, suggesting that it can be an effective approach in significantly reducing the
                                trainingflops.

This setup can also be viewed as transforming
                                tokenizer-based models into tokenizer-free ones, effectively converting a pre-trained
                                LLaMA 3.1 model into aBLT model. To provide a comprehensive comparison, we
                                include the original LLaMA 3.1 model trained on 15T tokens in Table5and evaluate it against theBLT derived from LLaMA
                                3. Our model experiences a slight performance decline on MMLU and HumanEval, but a more
                                significant drop on other tasks. This suggests that further work is needed to fully
                                leverage the pre-trained model and improve upon its performance, particularly in terms
                                of optimizing data mixtures and other hyperparameters.


7 Ablations and Discussion

In this section, we discuss ablations justifying architectural
                            choices forBLT and the
                            patching scheme and hyper-parameters for theBLT 8B parameter model trained on theBLT-1T dataset.

To study the effect of varying entropy model size
                                and context window length on scaling performance, we train byte-level entropy
                                transformer models of different model sizes between 1m and 100m parameters, with varying
                                context window lengths from 64 to 512. We plot bpb vs trainingflopscaling
                                law curves, created using our 400
‚Å¢
m



400
ùëö


400m
                                        

                                            400 italic_m and 1
‚Å¢
b



1
ùëè


1b
                                        
1
                                            italic_b BLT
                                models trained on the Llama-2 dataset and present them inFigure 8. We find that scaling
                                performance is positively correlated with both these dimensions of the entropy model,
                                with diminishing returns when we scale beyond 50m parameters.

We ablate the four different patching schemes,
                                introduced in Section2i.e. 1) Strided Patching with a stride
                                of 4 and 6, 2) Patching on whitepsace, 3) BPE Tokenizer patching based on the Llama 3
                                tokenizer, and 4) Entropy based patching using a small bytellm.

While dynamic patching reduces the effective
                                length of sequences, we control for the sequence length to maintain a similar context
                                length for all patching schemes. All the models see the same number of bytes in each
                                sequence during training and inference in expectation to prevent any confounding factors
                                from being able to model larger contexts. Figure6highlights the results of these
                                ablations. All the remaining patching schemes outperform static patching, with space
                                patching being a very close competitor to dynamic entropy based patching.

InTable 6, we present benchmark
                                evaluations forBLT models comparing tokenizer-based models,
                                space patching, and entropy-based patching, trained on the Llama 2 dataset for an
                                optimal number of steps. Although space patching is a simpler strategy that
                                does not involve running an entropy model on the fly during training, we find that the
                                gains we observed using entropy-based patching on scaling trends (Section5) do indeed carry forward even to
                                downstream benchmark tasks.

InTable 7, we ablate including
                                cross-attention at various points in the encoder and decoder ofBLT.
                                For the encoder cross-attention we test initializing the queries with 1) the same
                                learned embedding for every global state, 2) a hash embedding of the bytes in the patch,
                                and 3) pooling of the encoder hidden representation of the patch bytes at the given
                                encoder layer.

We find that using cross-attention in thedecoderis most
                                effective. In the encoder, there is a slight improvement in using cross-attention but
                                only with pooling initialization of queries. Additionally, we find that cross-attention
                                helps particularly on Common-Crawl and especially with larger patch sizes.

We ablate settings of 0, 100K, 200K and 400K
                                n-gram hash embedding vocabularies and present results in Table8. We find that hash embeddings help on
                                all domains, but particularly on Wikipedia and Github (0.04 bpb difference compared to
                                0.01 bpb difference after 15k steps at 8B).
                                At 8B scale going from 500K to 300K hashes changed performance by 0.001 bpb on 15k
                                steps. This indicates that hashes are vital to bringing the performance ofBLT to match
                                those of tokenizer based models, however, after 300K hashes, there are diminishing
                                returns. Additionally, it appears that the gains are largely complementary with
                                cross-attention as they provide improvements on different datasets.

In Table9, we ablate various settings for the
                                number of layers in the local encoder and decoder. When paired with hash n-gram
                                embeddings,BLT works well with an encoder that is extremely
                                light-weight i.e. just one layer, and with a heavier decoder.


8 Related Work

Character-Level RNNs:Character Language Modeling has been a
                            popular task ever since the early days of neural modelsowing
                            to their flexibility of modeling out of vocabulary words organically without resorting to
                            back-off methods.also
                            train a model that processes characters only on the input side using convolutional and
                            highway networks that feed into LSTM-based RNNs and are able to match performance with the
                            RNN based state-of-the-art language models of the time on English and outperform them on
                            morphologically rich languages, another sought-after advantage of character-level LLMs.do
                            machine comprehension using byte-level LSTM models that outperformed word-level models again
                            on morphologically-rich Turkish and Russian languages. Along similar lines,used
                            character-based convolutional models for classification tasks, which outperformed word-level
                            models for certain tasks.use hierarchical LSTM models using boundary-detectors at
                            each level to discover the latent hierarchy in text, to further improve performance on
                            character level language modeling. ByteNet byuses
                            CNN based layers on characters as opposed to attention for machine translation.

Character-Level Transformers:The development of transformer
                            models using attentiontogether with subword tokenization,
                            significantly improved the performance of neural models on language modeling and benchmark
                            tasks. However, word and sub-word units implicitly define an inductive bias for the level of
                            abstraction models should operate on. To combine the successes of transformer models with
                            the initial promising results on character language modeling,use
                            very deep transformers, and with the help of auxiliary losses, train transformer-based
                            models that outperformed previous LSTM based characterllms. However, they still saw
                            a significant gap from word level LLMs. GPT-2also
                            observed that on large scale datasets like the 1 billion word benchmark, byte-level LMs were
                            not competitive with word-level LMs.

Whiledemonstrated that byte-levelllms based on transformers can
                            outperform subword level LLMs with comparable parameters, the models take up much more
                            compute and take much longer to train. Similarly,train
                            a BERT model (CharFormer) that builds word representations by applying convolutions on
                            character embeddings, and demonstrate improvements on the medical domain, but they also
                            expend much more compute in doing so.develop CANINE, a 150M parameter encoder-only model that
                            operates directly on character sequences. CANINE uses a deep transformer stack at its core
                            similar in spirit to our global model, and a combination of a local transformer and strided
                            convolutions to downsample the input characters, and outperforms the equivalent token-level
                            encoder-only model (mBERT) on downstream multilingual tasks. ByT5explored approaches for byte-level encoder decoder models, that do not use any kind of
                            patching operations. While their model exhibited improved robustness to noise, and was
                            competitive with tokenizer-based models with 4x less data, the lack of patching meant that
                            the models needed to compute expensive attention operations over every byte, which was
                            extremely compute heavy. Directly modeling bytes instead of subword units increases the
                            sequence length of the input making it challenging to efficiently scale byte level models.
                            Recently, using the Mamba Architecture, which can maintain a fixed-size memory state over a very
                            large context length,train a byte-level Mamba architecture also without using
                            patching, and are able to outperform byte-level transformer models in aflopcontrolled setting at the
                            350M parameter scale in terms of bits-per-byte on several datasets.

Patching-based
                                approaches:The effective use of patching can bring down the otherwise inflated
                            number offlops expended by
                            byte-level LLMs while potentially retaining performance, and many works demonstrated initial
                            successes at a small scale of model size and number of training bytes.experiment with static patching based downsampling and upsampling and develop the hourglass
                            transformer which outperforms other byte-level baselines at the 150M scale.further improve this with the help of dynamic patching schemes, including a
                            boundary-predictor that is learned in an end-to-end fashion, a boundary-predictor supervised
                            using certain tokenizers, as well as an entropy-based patching model similar toBLT, and show that this
                            approach can outperform the vanilla transformers of the time on language modeling tasks at a
                            40M parameter scale on 400M tokens.investigate training on sequences compressed using
                            arithmetic coding to achieve compression rates beyond what BPE can achieve, and by using a
                            equal-info windows technique, are able to outperform byte-level baselines on language
                            modeling tasks, but underperform subword baselines.

Our work draws inspiration and is most closely related to MegaByte,
                            which is a decoder only causal LLM that uses a fixed static patching and concatenation of
                            representations to convert bytes to patches, and uses a local model on the decoder side to
                            convert from patches back into bytes. They demonstrate that MegaByte can match
                            tokenizer-based models at a 1B parameter scale on a dataset of 400B bytes. We ablate
                            MegaByte in all our experiments and find that static patching lags behind the current
                            state-of-the-art compute optimally trained tokenizer based models in aflopcontrolled setting and we
                            demonstrate howBLT bridges
                            this gap.make
                            the same observation about MegaByte and suggest extending the static patching method to
                            patching on whitespaces and other space-like bytes, and also add a local encoder model. They
                            find improvements over tokenized-based transformer models in a compute controlled setting on
                            some domains such as Github and arXiv at the 1B parameter scale. We also report experiments
                            with this model, and show that further architectural improvements are needed to scale up
                            byte-level models even further and truly match current state-of-the-art token-based models
                            such as Llama 3.


9 Limitations and Future Work

In this work, for the purposes of architectural choices, we train
                            models for the optimal number of steps as determined for Llama 3.
                            However, these scaling laws were calculated for BPE-level transformers and may lead to
                            suboptimal (data, parameter sizes) ratios in the case ofBLT. We leave for future work
                            the calculation of scaling laws forBLT potentially leading to even more favorable scaling trends for
                            our architecture. Additionally, many of these experiments were conducted at scales upto 1B
                            parameters, and it is possible for the optimal architectural choices to change as we scale
                            to 8B parameters and beyond, which may unlock improved performance for larger scales.

Existing transformer libraries and codebases are designed to be
                            highly efficient for tokenizer-based transformer architectures. While we present theoreticalflopmatched experiments and
                            also use certain efficient implementations (such as FlexAttention) to handle layers that
                            deviate from the vanilla transformer architecture, our implementations may yet not be at
                            parity with tokenizer-based models in terms of wall-clock time and may benefit from further
                            optimizations.

WhileBLT uses a separately trained entropy model for patching, learning
                            the patching model in an end-to-end fashion can be an interesting direction for future work.
                            In Section6.2, we present initial experiments showing
                            indications of success for ‚Äúbyte-ifying‚Äù tokenizer-based models such as Llama 3 that are
                            trained on more than 10T tokens, by initializing and freezing the global transformer with
                            their weights. Further work in this direction may uncover methods that not only retain the
                            benefits of bytefying, but also push performance beyond that of these tokenizer-based models
                            without training them from scratch.


10 Conclusion

This paper presents the Byte Latent Transformer (BLT), a new architecture that
                            redefines the conventional dependency on fixed-vocabulary tokenization in large language
                            models. By introducing a dynamic, learnable method for grouping bytes into patches,BLT effectively allocates
                            computational resources based on data complexity, leading to significant improvements in
                            both efficiency and robustness. Our extensive scaling study demonstrates thatBLT models can match the
                            performance of tokenization-based models like Llama 3 at scales up to 8B and 4T bytes, and
                            can trade minor losses in evaluation metrics for up to 50% reductions in inferenceflops.
                            Furthermore,BLT unlocks a
                            new dimension for scaling, allowing simultaneous increases in model and patch size within a
                            fixed inference budget. This new paradigm becomes advantageous for compute regimes commonly
                            encountered in practical settings.
                            While directly engaging with raw byte data,BLT also improves the model‚Äôs ability to handle the long-tail of
                            data, offering significant improvements in robustness to noisy inputs and a deeper
                            understanding of sub-word structures.
                            Overall, these results positionBLT as a promising alternative to traditional tokenization-based
                            approaches, providing a scalable and robust framework for more efficient and adaptable
                            language models.


Acknowledgements

We would like to thank Kalyan Saladi for help with everything
                            relating to pre-training infrastructure; Gabriel Synnaeve, Ammar Rizvi, Jacob Kahn, Michel
                            Meyer for helping organize resources for scaling upBLT; Badr Youbi Idirissi,
                            Mathurin Videau, and Jade Copet for invaluable discussions and feedback aboutBLT, for access to the Lingua
                            framework for open-sourcing code forBLT, and for help preparing theBLT-1T dataset used in this
                            paper; Omer Levy, who was actively involved in the early stages of the project and provided
                            valuable feedback and ideas; Driss Guessous for help with FlexAttention; and Sida Wang,
                            Melanie Sclar, Amanda Bertsch, and Hunter Lang for feedback and discussions.


Contributors

In this section, we list individual contributions.

Artidoro Pagnoni, Srinivasan Iyer, Ramakanth
                                Pasunuru, Pedro Rodriguez, John Nguyen, Gargi Ghosh (Project Lead)

Mike Lewis, Ari Holtzman, Luke Zettlemoyer

Jason Weston, Benjamin Muller, Margaret Li,
                                Chunting Zhou, Lili Yu


11 Model Hyper Parameters

Table10shows different hyper parameter settings
                            forBLT models.


12 FLOPs Equations

Here, we provide the equations used forflopcomputation for the
                            forward-pass of transformer andBLT models based on. We
                            assume that the backward pass uses twice as muchflops as the forward pass.

For a transformer model with l
ùëô

l
italic_l layers, hidden dimension h
‚Ñé

h
italic_h , context length m
ùëö

m
italic_m , n

h
‚Å¢
e
‚Å¢
a
‚Å¢
d
‚Å¢
s




                                                subscript
ùëõ


‚Ñé
ùëí
ùëé
ùëë
ùë†



n_heads
italic_n
                                        start_POSTSUBSCRIPT italic_h italic_e italic_a italic_d italic_s
                                        end_POSTSUBSCRIPT attention heads of dimension h
k



                                                subscript
‚Ñé
ùëò


h_k
italic_h
                                        start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , and a feed-forward multipler of d

f
‚Å¢
f




                                                subscript
ùëë


ùëì
ùëì



d_ff
italic_d
                                        start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT , we computeflops
                            as:

ForBLT models, we use the above-mentioned primitives together with
                            theflops equation from
                            Section4.5to compute totalflops.


13 Rolling Polynomial Hashing

Given a byte n
ùëõ

n
italic_n -gram g

i
,
n


=

{

b
                                                


i
‚àí
n

+
1


,
‚Ä¶
                                            
,

b
                                                
i
                                                

}





subscript
ùëî

ùëñ
ùëõ




subscript
ùëè




ùëñ
ùëõ

1


‚Ä¶

subscript
ùëè
ùëñ





                                        g_i,n=\{b_i-n+1,\ldots,b_i\}
italic_g
                                        start_POSTSUBSCRIPT italic_i , italic_n end_POSTSUBSCRIPT = { italic_b
                                        start_POSTSUBSCRIPT italic_i - italic_n + 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_b
                                        start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } , the rolling polynomial hash of g

i
,
n




                                                subscript
ùëî

ùëñ
ùëõ



g_i,n
italic_g
                                        start_POSTSUBSCRIPT italic_i , italic_n end_POSTSUBSCRIPT is defined as:

Where a
ùëé

a
italic_a is chosen to be a 10-digit prime number.


14 Frequency-based n-gram Embedddings

Prior to using hash n-gram embeddings in the finalBLT architecture, we also
                            experimented with frequency-based n-gram embeddings. For each n
‚àà

{
1
,
2
,
3
,
4
,
5
,
6
,
7
,
8
}




ùëõ

1
                                                
2
                                                
3
                                                
4
                                                
5
                                                
6
                                                
7
                                                
8
                                                



n\in\{1,2,3,4,5,6,7,8\}
                                    
italic_n ‚àà { 1 , 2
                                        , 3 , 4 , 5 , 6 , 7 , 8 } there is an embedding matrix E
n

n
‚Å¢
g
‚Å¢
r
‚Å¢
a
‚Å¢
m




                                                superscript

subscript
ùê∏
ùëõ



ùëõ
ùëî
ùëü
ùëé
ùëö



E_n^ngram
                                    
italic_E
                                        start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n
                                        italic_g italic_r italic_a italic_m end_POSTSUPERSCRIPT that contains the most frequent byte-grams for the given n
ùëõ

n
italic_n .
                            Since it is intractable to store embeddings as n
ùëõ

n
italic_n grows, we only store embeddings for the most frequent 100
,
000


100
000


100,000
100 , 000 byte-grams for each byte-gram.
                            If a particular position i
ùëñ

i
italic_i includes an n
ùëõ

n
italic_n -gram present in the corresponding the embedding matrix, then this embedding is
                            passed to the next step, encoder multi-headed cross-attention. If a byte-gram is infrequent
                            and therefore not in the matrix, then its embedding is obtained from encoder hash embeddings
                            instead.

Since frequency-based n
ùëõ

n
italic_n -grams are limited by the vocabulary of the n-gram tables with infrequent n-grams not
                            being represented at all, we subsequently moved to hash-based n
ùëõ

n
italic_n -gram embeddings. SeeTable 12for a comparison of hash and
                            frequency based n-gram embeddings.


15 Entropy Patching Example from MMLU

We illustrate how a few-shot example from a downstream task i.e.
                            MMLU, is
                            patched using an entropy-model trained for use withBLT models in Figure9. Directly using the entropy model with the
                            full-context window causes repetitive patterns to be heavily patched. For example, ‚Äú10
                            times, with an rms deviation of about‚Äù in the MMLU query is patched frequently the first
                            time it is encountered, but is part of very large patches the next three times, which,
                            although inference efficient, maybe undesirable for reasoning. One method that we use to
                            avoid such a ‚Äúentropy‚Äù drift is by resetting the entropy context with new lines and using a
                            approximate monotonicity constraint (see Section4.4).
